 \documentclass[12pt]{amsart}

\usepackage{/Users/ethanjaffe/Documents/myMacros}
\usepackage{/Users/ethanjaffe/Documents/mySettings}
\usepackage{chngcntr}
\DeclareMathOperator{\range}{range}
\newcommand{\D}{\mathbf D}

\title{Exterior products of Hilbert spaces and the Fredolm determinant}
\author{Ethan Y. Jaffe}
\date{}


\begin{document}
\maketitle
\numberwithin{equation}{section}
Let $H$ be a (separable) Hilbert space.\footnote{The separability assumption is mostly for notational convenience and to avoid having to talk about strongly convergent nets of projections rather than more pedestrian convergent sequences.} In this note we talk about the exterior products $\Lambda^k H$. The main application of this will be to define the Fredolm determinant $\det(1+A)$, for $A$ trace class and to examine its properties.

\section{Exterior Products}

Consider the space $\Lambda^k H$ for $k \in N$, the (algebraic) vector space span of $k$-blades $\{v_1\wedge \cdots \wedge v_k\: v_1,\ldots,v_k \in H\}$. Formally, $\Lambda^k H$ is the quotient of the algebraic tensor product $H^{\otimes k}$ by the ideal generated by $\{v_1\otimes \cdots \otimes v_k \: v_i = v_j\text{ for some }i \neq j\}$. Observe that $\Lambda^k H$ is by definition characterized by the property that whenever $\Phi:H^k \to X$ is an alternating multilinear map of vector spaces, then there exists a unique map $\Lambda^k H \to X$ given by
\[\widetilde{\Phi}(v_1\wedge \cdots \wedge v_k) = \Phi(v_1,\ldots,v_k)\] on $k$-blades. We equip $\Lambda^k H$ with an inner product defined by
\begin{equation}\label{inner}\langle v_1\wedge \cdots \wedge v_k,w_1 \wedge \cdots \wedge w_k\rangle = \det(\langle v_i,w_j\rangle)\end{equation}
on $k$-blades and extending by linearity. Here, $\det(a_{ij})$ denotes the determinant of the matrix whose $(i,j)^{th}$ entry is $a_{ij}$. The space $\Lambda^k H$ is not a Hilbert space, so we hereafter replace $\Lambda^k H$ with its complition under this inner product, which is a Hilbert space. When we need to refer to the original, algebraic space, we will use the notation $\widetilde{\Lambda^k} H$. 

The inner product \eqref{inner} is not obviously well-defined, as $k$-blades don't have unique representations in $\widetilde{\Lambda^k}H$ (in fact a $k$-blade may be written as the sum of other $k$-blades!). We need to prove that it is well-defined.
\begin{lem}The inner product \eqref{inner} is well-defined on $\widetilde{\Lambda^k}H$, and hence defines an actual inner product.\end{lem}
\begin{proof}
 Fix $w_1,\ldots, w_k \in H$, and consider the map $\Phi:H^k \to \C$ defined by
\[\Phi(v_1,\ldots,v_k) = \det(\langle v_i,w_j\rangle).\] Then $\Phi$ is is multilinear and alternating, and so by definition descends to a well-defined map from $\widetilde{\Lambda^k}H \to \C$. This shows that $\langle v_1\wedge \cdots \wedge v_k,w_1 \wedge \cdots \wedge w_k\rangle$ is well-defined in the first argument. Since it is clearly conjugate symmetric, it is well-defned in the second argument, and so is well-defined overall.
\end{proof}


\begin{lem}[Properties of $\Lambda^k H$.]\label{props}The following hold:
\begin{romanumerate}
\item Suppose that for $1 \leq i \leq k$, $v_i^n$ is a sequence of vectors in $H$ converging to $v_i \in H$. Then
\[v_1^n \wedge \cdots \wedge v_k^n \to v_1 \wedge \cdots \wedge v_k;\]
\item The span of the $k$-blades $\{v_1\wedge \cdots \wedge v_k\}$ is dense in $\Lambda^k H$;
\item If $e_1,e_2,\ldots$ is an orthonormal basis of $H$, then $\Lambda^k H$ has an orthonormal basis of the form $\beta = \{e_{i_1}\wedge \cdots \wedge e_{i_k}\: i_1 < \cdots < i_k\}$.
\end{romanumerate} \end{lem}
\begin{proof}
Let us prove (i). We compute
\[\norm{v_1^n \wedge \cdots \wedge v_k^n-v_1 \wedge \cdots \wedge v_k}^2 = \det(\langle v_i^n,v_j^n\rangle) + \det(\langle v_i,v_j\rangle) - \det(\langle v_i^n,v_j)-\det(\langle v_i,v_j^n\rangle).\]
Since the determinant of a matrix is continuous in its entries, as $n \to \infty$ this converges to
\[\det(\langle v_i,v_j\rangle) + \det(\langle v_i,v_j\rangle) - \det(\langle v_i,v_j\rangle) - \det(\langle v_i,v_j\rangle) = 0.\]

(ii) is true since by definition $\widetilde{\Lambda^k}H$ is the span of $k$-blades, and this space is dense in its completion, $\Lambda^k H$.

Now let us prove (iii). It is clear that $\beta$ is an orthonormal set. We show that it is a basis. Suppose $v_1,\ldots,v_k \in H$ and for all $i$
\[v_i = \lim_{n \to \infty} w_i^n\] for $w_i^n \in \vspan\{e_1,e_2,\ldots\}$ is in the vector space span (i.e.\ is a finite linear combination). From (i), we know that
\[w_1^n\wedge\cdots\wedge w_k^n \to v_1\wedge\cdots v_k.\] Each $k$-blade $w_1^n\wedge\cdots\wedge w_k^n$ belongs to $\vspan \beta$, so this shows that $\vspan \beta$ is dense in $\Lambda^k H$, which is sufficient.
\end{proof}

We now show that a bounded linear map $A$ can be used to define an operator $\Lambda^k A$ on $\Lambda^k H$, and that this assignment is functorial.
\begin{thm}\label{extends}Let $A:H \to H$ be bounded. Then for each $k$ there exists a unique bounded operator $\Lambda^k A: \Lambda^k H \to \Lambda^k H$ such that $\Lambda^k A$ acts on $k$-blades by
\[(\Lambda^k A)(v_1\wedge \cdots v_k) = Av_1 \wedge \cdots \wedge Av_k.\] Furthermore, $\norm{\Lambda^k A} \leq \norm{A}^k$, and the map $\Lambda^k:B(H) \to B(\Lambda^k H)$ is continuous. Explicitly, for $A,B \in B(H)$ (and $k \geq 1$)
\begin{equation}\label{ctsb}\norm{\Lambda^k A - \Lambda^k B} \leq k\norm{A-B}\max(\norm{A},\norm{B})^{k-1}.\end{equation}

The map $\Lambda^k$ is functorial in the following sense:
\begin{romanumerate}
\item $\Lambda^k (AB) = \Lambda^k A \Lambda^k B$ for $A,B$ bounded;
\item if $A$ is invertible, then $\Lambda^k A$ is invertible with inverse $\Lambda^k A^{-1}$;
\item $(\Lambda^k A)^\ast = \Lambda^k A^\ast$;
\item if $\Pi:H \to K$ is the orthogonal projection onto $K$, then $\Lambda^k \Pi$ is the orthogonal projection onto $\Lambda^k K$, the closure of the span of $k$-blades $\{v_1 \wedge \cdots \wedge v_k\: v_i \in K, 1 \leq i \leq k\}$;
\item if $A$ is positive, then $\Lambda^k A$ is positive;
\item $|\Lambda^k A| = \Lambda^k |A|$.
\end{romanumerate}


If $A$ is additionally trace class, then $\Lambda^k A$ is also trace class, and
\[\norm{\Lambda^k A}_1 \leq \frac{\norm{A}_1^k}{k!}.\] Futhermore, the map $\Lambda^k:\ell^1(H) \to \ell^1(\Lambda^k H)$ is continuous, with explicit bounds for $A,B$ trace class (for $k \geq 1$)
\begin{equation}\label{ctsbp}\norm{\Lambda^k A - \Lambda^k B}_1 \leq \norm{A-B}_1\frac{\max(\norm{A}_1,\norm{B}_1)^{k-1}}{(k-1)!}.\end{equation}
\end{thm}

To prove this, we need the following technical lemma:
\begin{lem}\label{technical}Let $H$ be a Hilbert space and suppose $X \subseteq H$ is dense. Let $A_n$ be a sequence of uniformly bounded operators such that $A_nx$ converges pointwise for each $x \in X$. Then $A_n$ converges strongly to a bounded operator $A$, and $\norm{A} \leq \limsup \norm{A_n}$.\end{lem}
\begin{proof}We show that for all $v \in H$, $A_nv$ is Cauchy, and thus $A_n$ converges strongly to a linear map $A$. Fix $\epsilon > 0$. By density and uniform boundedness, there exists $x \in X$ such that for all $n \in \N$, $\norm{A_nv-A_nx} < \epsilon/3$. Now for $N$ large, if $n,m> N$, we may assume that $\norm{A_nx-A_mx} < \epsilon/3$. Thus if $n,m > N$
\[\norm{A_nv-A_mv} \leq \norm{A_nv-A_nx} + \norm{A_nx-A_mx} + \norm{A_mv-A_mx} < \epsilon.\]
For $v \in H$, and $\espilon > 0$, again choose $x$ with $\norm{A_nx-A_nv)} \leq \epsilon$. Then
\[\norm{Av} = \lim_{n \to infty} \norm{A_nv} \leq \limsup_{n \to \infty} \norm{A_nv-A_nx} + \norm{A_nx} \leq \epsilon + \limsup_{n \to \infty}\norm{A}\norm{x}.\]
Since $\norm{x} \leq \epsilon + \norm{v}$, it follows that
\[\norm{Av} \leq (1+\limsup_{n \to \infty}\norm{A})(\epsilon) + \limsup_{n \to \infty}\norm{A}\norm{v}.\]
Taking $\epsilon \to 0$ shows that $\norm{Av} \leq \limsup_{n \to \infty}\norm{A}\norm{v}$, which shows that $A$ is bounded and $\norm{A} \leq \limsup \norm{A_n}$.\end{proof}


\begin{proof}[Proof of \cref{extends}]
This theorem has many different parts, so we prove them separately.
\textbf{Part 1: uniqueness and functoriality.} Since the span of $k$-blades is dense, uniqueness follows immediately. By density and linearity, functoriality will follow if we can check each statement on a basis. For (i), observe that for any $k$-blade $v_1\wedge \cdots \wedge v_k$,
\begin{align*}
\Lambda^k (AB)(v_1\wedge \cdots \wedge v_k) &= (ABv_1)\wedge \cdots \wedge (ABv_k)\\
&= \Lambda^k A((Bv_1)\wedge \cdots \wedge (Bv_k))\\
&= \Lambda^k A\Lambda^k B(v_1\wedge \cdots \wedge v_k).\end{align*}
Property (ii) follows from (i), since 
\[\Lambda^k A^{-1}\Lambda^k A = \Lambda^k 1 = \Lambda^k A \Lambda^k A^{-1},\] and $\Lambda^k 1$ is certainly the identity since it maps any $k$-blade to itself.
For (iii), observe that for any other $k$-blade $w_1\wedge \cdots \wedge w_k$,
\begin{align*}
\langle \Lambda^k A(v_1\wedge \cdots \wedge v_k),w_1\wedge \cdots \wedge w_k\rangle &= \det(\langle Av_i,w_j\rangle) = \det(\langle v_i,Aw_j\rangle)\\
&= \langle v_1\wedge \cdots \wedge v_k,\Lambda^k A w_1\wedge \cdots \wedge w_k.\end{align*}

For (iv), observe that $\Lambda^k \Pi$ is self-adjoint (from (iii)) and idempotent (from (i)). Thus $\Lambda^k \Pi$ is the orthogonal projection onto its range. Certainly $\Lambda^k K \subseteq \range(\Lambda^k \Pi)$, since $\Lambda^k \Pi$ acts as the identity on the wedge product of vectors in $K$. We now show that its range is contained in $\Lambda^k K$. If $v = v_1\wedge \cdots \wedge v_k$ is a $k$-blade, then we may write $v_i = u_i + w_i$ where $u_i \in K$ and $w_i \bot K$. Thus
\[v = u_1 \wedge \cdots \wedge u_k + w,\] where $w$ is a sum of wedges at least one of whose factors is orthogonal to $K$. Thus
\[\Lambda^k \Pi v = u_1 \wedge \cdots \wedge u_k + 0 \in \Lambda^k K.\] It follows that the range of $\Lambda^k \Pi$ on the span of $k$-blades is contained in $\Lambda^k K$, and hence the range of $\Lambda^k \Pi$ on all of $\Lambda^k H$ is contained in $\Lambda^k H$, since the span of $k$-blades is dense and $\Lambda^k K$ is closed by definition.

For (v), first assume that $A$ is compact. Suppose $e_1,e_2,\ldots$ is an orthonormal basis for $H$ of eigenvectors of $|A|$. Then $\{e_{i_1}\wedge \cdots \wedge e_{i_k}\: i_1 < \cdots < i_k\}$ is an orthonormal basis of eigenvectors of $\Lambda^k A$. Since each associated eigenvalue is positive, it follows that $\Lambda^k A$ is positive. If $A$ is not compact, then fix any orthonormal basis $e_1,e_2,\ldots$ of $H$, and let $\Pi_n$ be the orthogonal projection onto $\vspan\{e_1,\ldots,e_n\}$. Then $\Pi_nA\Pi_n$ is positive, and so $\Lambda^k \Pi_n \Lambda^k A \Lambda^k \Pi_n$ is positive. The operator $\Lambda_k \Pi_n$ is by (iv) the orthogonal projection onto $\vspan\{e_{i_1}\wedge e_{i_k}\: i_1 < \cdots i_k \leq n\}$, and thus converges strongly to $1$. Thus $\Lambda^k \Pi_n \Lambda^k A \Lambda^k \Pi_n$ converges strongly to $\Lambda^k A$. Since a strong limit of positive operators is positive, $\Lambda^k A$ is also positive.

For (vi), observe first that
\[(\Lambda^k |A|)^2 = \Lambda^k |A|^2 = \Lambda^k A^\ast A = (\Lambda^k A)^\ast\Lambda^k A,\] and $\Lambda^k |A|$ is positive. Thus $\Lambda^k |A|$ is a positive square root of $(\Lambda^k A)^\ast\Lambda^k A = |\Lambda^k A|^2$, and thus must coincide with $|\Lambda^k A|$.\footnote{Indeed, if $P$ and $Q$ are positive operators on a Hilbert space, and $Q^2 = P$, then $Q = \sqrt{P}$. To show this, suppose $a > 0$ is large enough so that $\sigma(P) \subseteq [0,a]$ and $\sigma(Q) \subseteq [0,\sqrt{a}]$. Suppose $p_n(x)$ are polynomials converging to $\sqrt{x}$ uniformly on $[0,a]$. Then $p_n(Q^2) = p_n(P) \to \sqrt{P}$. On the other hand, $p_n(x^2) \to x$ on $[0,\sqrt{a}]$, and so $p_n(Q^2) \to Q$.}


\textbf{Part 2: Existence.} Now let us show existence. We first suppose that $A$ is positive and compact. Let $e_1,e_2,\ldots$ be an orthonormal basis of eigenvctors of $A$, and suppose $Ae_i = \lambda_i e_i$. Suppose $\lambda_1$ is the largest eigenvalue. Let $\beta = \{e_{i_1}\wedge \cdots \wedge e_{i_k}\: i_1 < \cdots < i_k\}$. We first define a map $B:\vspan \beta \to \Lambda^k H$, and then show it is bounded, and thus $B$ extends to a bounded map $B:\Lambda^k H \to \Lambda^k H$. We then show that \[B(v_1\wedge \cdots \wedge v_k) = Av_1 \wedge \cdots \wedge Av_k,\] and thus we can define $\Lambda^k A = B$. For $\alpha = (\alpha_1,\ldots,\alpha_k)$ an increasing $k$-tuple, set $e_\alpha = e_{\alpha_1} \wedge \cdots \wedge e_{\alpha_k}$, and $\lambda_\alpha = \lambda_{\alpha_1}\cdots\lambda_{\alpha_k}$. Define $B(e_\alpha) = \lambda_\alpha e_\alpha$, and then extend by linearity. Thus, if $v = \sum a_\alpha e_\alpha$ is a finite linear combination,
\[\norm{Bv}^2 = \sum |a_\alpha|^2\norm{Be_\alpha}^2 = \sum |a_\alpha|^2 \lambda_\alpha^2 \leq \lambda_1^{2k}\norm{v}^2,\] and so $B$ extends to a bounded operator. In fact, this shows that $\norm{B} \leq \lambda_1^k = \norm{A}^k$.

If $v_1,\ldots,v_k \in H$ are finite linear combinations of the $e_i$, then it is easy to check that
\[B(v_1\wedge \cdots \wedge v_k) = Av_1 \wedge  \cdots \cdots\wedge Av_k.\] Indeed, suppose $v_i = \sum a_i^je_j$ for all $i$. Let $N$ be the large index such that $a_i^N$ is nonzero for some $i$. Let $T_k$ denote the set of injective maps from $\{1,\ldots,k\} \to \{1,\ldots,N\}$. Then
\begin{align*}
B(v_1\wedge \cdots \wedge v_k) &= B\left(\sum_{\sigma \in T_k} \prod_{i=1}^k a_i^{\sigma(i)}\bigwedge_{i=1}^k e_{\sigma(i)}\right)\\
&=  \sum_{\sigma \in T_k}\prod_{i=1}^k a_i^{\sigma(i)}\bigwedge_{i=1}^k \lambda_i e_{\sigma(i)}\\
&= \sum_{\sigma \in T_k} \prod_{i=1}^k a_i^{\sigma(i)}\bigwedge_{i=1}^k Ae_{\alpha(\sigma)_i}\\
&= Av_1 \wedge  \cdots \cdots\wedge Av_k.\end{align*}
Now if $v_1,\ldots,v_k$ are not finite linear combinations, then we can write them as a limit of finite linear combinations, and use the fact that $B$ and $A$ are bounded, together with \cref{props}.

Now assume that $A$ is positive, but not compact. If $e_1,e_2,\ldots$ is any orthonormal basis of $H$, let $\Pi_n$ denote the orthogonal projection onto $\vspan\{e_1,\ldots,e_n\}$. Then $\Pi_n A \Pi_n$ is positive and compact, and so $\Lambda^k (\Pi_n A \Pi_n)$ exists. Using \cref{props} and the definition of $\Lambda^k(\Pi_n A \Pi_n)$ on $k$-blades, it follows that for any $k$-blade $v_1\wedge \cdots \wedge v_k$ 
\[\Lambda^k (\Pi_n A \Pi_n)v_1\wedge \cdots \wedge v_k = (\Pi_n A \Pi_n v_1)\wedge \cdots \wedge (\Pi_n A \Pi_n A v_k) \to Av_1 \wedge \cdots \wedge Av_k.\] Thus by linearity $\Lambda^k (\Pi_n A \Pi_n)$ converges pointwise on the span of $k$-blades. Also, \[\norm{\Lambda^k\Pi_n A \Pi_n} \leq \norm{\Pi_nA\Pi_n}^k \leq \norm{A}^k\] for each $n$. Thus, since the span of $k$-blades is dense, by \cref{technical}, $\Lambda^k (\Pi_n A \Pi_n)$ converges strongly to some operator $B$. Since we have already shown that
\[B(v_1\wedge \cdots \wedge v_k) = \lim_{n \to \infty} \Lambda^k(\Pi_n A \Pi_n)(v_1\wedge \cdots \wedge v_k) = Av_1 \wedge  \cdots \cdots\wedge Av_k\] for any $k$-blade, we may set $\Lambda^k A = B$.

Now let $A$ be a partial isometry. Let $e_1,e_2,\ldots$ be an orthonormal basis of $H$ which is the result of taking the union of an orthonormal basis of $\ker A$ and $\ker A^\bot$ (and then relabelling), and let $\beta$ be as above. As above, we first define a map $B:\vspan\beta \to \Lambda^k H$, show it is bounded, and that it behaves correctly on $k$-blades. Define 
\[B(e_\alpha) = Ae_{\alpha_1}\wedge \cdots \wedge Ae_{\alpha_n}.\] If $\alpha$ and $\alpha'$ are increasing $k$-tuples, then
\[\langle Be_\alpha,Be_{\alpha'}\rangle = \det (\langle A^\ast A e_{\alpha_i},e_{\alpha'_j}\rangle).\]
Now $A^\ast A$ is precisely the projection onto $\ker A^\bot$. Thus, if any $e_{\alpha_i} \in \ker A$, $\langle Be_\alpha,Be_\alpha'\rangle  = 0$. Otherwise (i.e.\ all $e_{\alpha_i}$ are in $\ker A^\bot$), it is equal to 
\[\det (\langle e_{\alpha_i},e_{\alpha'_j}\rangle) = \langle e_\alpha,e_{\alpha'}\rangle.\]
Now, if $v = \sum a_\alpha e_\alpha$ is a finite linear combination, let $S$ be the collection of those $\alpha$ such all $e_{\alpha_i} \in \ker A^\bot$. Then
\begin{align*}
\norm{Bv}^2 &= \sum_{\alpha,\alpha'}a_\alpha\overline{a_{\alpha'}}\langle Be_\alpha,Be_\alpha'\rangle\\
&= \sum_{\alpha \in S,\alpha'}a_\alpha\overline{a_{\alpha'}}\langle e_\alpha,e_{\alpha'}\rangle\\
&= \sum_{\alpha \in S} |a_\alpha|^2 \leq \norm{v}^2.\end{align*}
It follows that $B$ is bounded and has norm precisely $1$ (which is of course also $\norm{A}^k)$. The proof that $B$ behaves correctly on $k$-blades is the same as in the case that $A$ is compact and positive. Thus in this case, too, can we set $\Lambda^k A = B$.

Now for the general case. Suppose $A$ is bounded. Write $A = U|A|$ the polar decomposition, where $U$ is a partial isometry and $|A|$ is positive. Define
\[\Lambda^k A = \Lambda^k U \Lambda^k |A|,\] both factors of which exist. We need to show that $\Lambda^k A$ behaves properly on $k$-blades. But this is obvious, as for any $k$-blade $v_1\wedge \cdots \wedge v_k$,
\[\Lambda^k U \Lambda^k |A|v_1\wedge \cdots \wedge v_k = \Lambda^k U(|A|v_1 \wedge \cdots \wedge |A|v_k) = (U|A|v_1)\wedge \cdots \wedge (U|A|v_k).\]
Certainly \[\norm{\Lambda^k A} \leq \norm{\Lambda^k U}\norm{\Lambda^k |A|} \leq \norm{|A|}^k = \norm{A}^k.\]

\textbf{Part 4: Continuity.} Suppose $A$, $B$ are bounded operators. Let $e_1,e_2,\ldots$ be an orthonormal basis of $H$, and let $\Pi_n$ denote the projection on $\{e_1,\ldots,e_n\}$. Since $\Lambda^k \Pi_n$ is the projection onto $\vspan\{e_{i_1}\wedge e_{i_k}\: i_1 < \cdots i_k \leq n\}$, $\Lambda^k \Pi_n(\Lambda^k A-\Lambda^k B)\Lambda^k \Pi_n$ converges in the strong operator topology $\Lambda^k A-\Lambda^k B$. Since the operator norm is lower semicontinuous in the strong operator topology, it suffices to prove \eqref{ctsb} with $A$ and $B$ replaced by $\Pi_n A\Pi_n$ and $\Pi_n B \Pi_n$, respectively. In other words, we may assume that $H$ is finite-dimensional with $\dim H = n$ (and hence $k \leq n$ since the spaces $\Lambda^k H = 0$ for $k > n$). For $t \in [0,1]$, let $C(t) = tA + (1-t)B$. For $\alpha$ an increasing $k$-tuple and $v \in \Lambda^{k}H$, define
\[\gamma_{\alpha,v}(t) = \langle \Lambda^kC(t)e_\alpha,v\rangle,\]
which is smooth on $[0,1]$ (since $v$ may be expanded in a finite basis of $\Lambda^k H$).
In particular, 
\begin{equation}\label{FTC}\langle (\Lambda^kA - \Lambda^k B)e_\alpha,v\rangle = \gamma_{\alpha,v}(1)-\gamma_{\alpha,v}(0) = \int_0^1 \gamma_{\alpha,v}'(t)\ dt.\end{equation}
We now compute $\gamma'_{\alpha,v}$. For $1 \leq i \leq k$, denote by $\widehat{e_{\alpha_i}}$ the wedge of all $e_{\alpha_i}$ (in order) \emph{except} $e_{\alpha_i}$. Write $A-B = V|A-B|$. Since $H$ is finite-dimensional, we may assume that $V$ is unitary. We may also assume (by the spectral theorem) that the basis $\{e_1,\ldots,e_n\}$ of $H$ is a basis of eigenvectors for $|A-B|$, with eigenvalues $\lambda_i \geq 0$. Write $Ve_i = f_i$, so that $\{f_1,\ldots,f_n\}$ is an orthonormal basis. Use the notation $f_{\alpha} = f_{\alpha_1} \wedge \cdots \wedge f_{\alpha_\ell}$ for an increasing $\ell$-tuple $\alpha$.
Using that the wedge product is continuous, it is easy to check that
\begin{align*}
\gamma'_{\alpha,v}(t) &= \sum_{i=1}^k \langle e_{\alpha_1}\wedge \cdots \wedge_{e_{\alpha_{i-1}}}\wedge C'(t)e_{\alpha_i}\wedge e_{\alpha_{i+1}}\wedge \cdots \wedge e_{\alpha_k},v\rangle\\
&= \sum_{i=1}^k (-1)^{i+1} \langle (A-B)e_{\alpha_i} \wedge \Lambda^{k-1}C(t)\widehat{e_{\alpha_i}},v\rangle\\
&= \sum_{i=1}^k(-1)^{i+1}\lambda_{\alpha_i}\langle f_{\alpha_i}\wedge \Lambda^{k-1}C(t)\widehat{e_{\alpha_i}},v\rangle.\end{align*}
For all $j$, the wedge map $f_j\wedge\:\Lambda^{k-1}H \to \Lambda^k H$ has norm $1$. Let $\iota_{f_j}$ denote its adjoint which also has norm $1$. Then we can rewrite the previous display as
\begin{equation}\label{der}\gamma'_{\alpha,v}(t) = \sum_{i=1}^k(-1)^{i+1}\lambda_{\alpha_i}\langle \Lambda^{k-1}C(t)\widehat{e_{\alpha_i}},\iota_{f_{\alpha_i}}v\rangle.\end{equation}
For $v \in \Lambda^k H$, let $v_\alpha$ be the coefficients in the expansion $v = \sum_{\alpha} v_\alpha e_\alpha$. Then, from \eqref{FTC},
\begin{align}
\label{ff}
\begin{split}
\norm{\Lambda^k A - \Lambda^k B} &= \sup_{\norm{v} = \norm{w} = 1}\left|\sum_\alpha v_\alpha\langle (\Lambda^k A-\Lambda^kB)e_\alpha,w\rangle\right|\\
&\leq \sup_{\norm{v} = \norm{w} = 1} \int_0^1 \left|\sum_\alpha v_\alpha\gamma_{\alpha,w}'(t)\ dt\right| dt.
\end{split}\end{align}
Fix some $v,w$ with $\norm{v} = \norm{w} = 1$. Plugging in \eqref{der} for $\gamma'_{\alpha,w}(t)$ and applying Cauchy-Schwarz inequality yields
\begin{equation}\label{CS}\left|\sum v_\alpha\gamma_{\alpha,w}'(t)\ dt\right| = \left|\sum_{\alpha} v_\alpha\gamma'_{\alpha,v}(t)\right| \leq \left(\sum_{\alpha,i}|v_\alpha|^2|\lambda_{\alpha_i}|^2\right)^{1/2}\left(\sum_{\alpha,i}| \Lambda^{k-1}C(t)\widehat{e_{\alpha_i}},\iota_{f_{\alpha_i}}w\rangle|^2\right)^{1/2}.\end{equation}
The first factor is bounded by \[\sqrt{k}(\sup \lambda_i)\norm{v} = \sqrt{k}\norm{A-B}.\]
For the second, we may rewrite the sum instead over all pairs $(j,\beta)$, where $1 \leq j \leq n$, and $\beta$ is an increasing $(k-1)$-tuple none of whose terms is $j$. This yields
\[\sum_{\alpha,i}|\langle \Lambda^{k-1}C(t)\widehat{e_{\alpha_i}},\iota_{f_{\alpha_i}}v\rangle|^2 = \sum_{\beta,j}|\langle e_{\beta}, \Lambda^{k-1}C(t)^{\ast}\iota_{f_{j}}w\rangle|^2.\] Taking the sum first over $\beta$, one bounds this by
\[\sum_j \norm{\Lambda^{k-1}C(t)^\ast\iota_{f_j}w}^2 \leq \norm{\Lambda^{k-1}C(t)^\ast}^2\sum_j \norm{\iota_{f_j}w}^2.\]
The first factor is bounded by $\norm{C(t)^\ast}^{2(k-1)} = \norm{C(t)}^{2(k-1)}$. For the second factor, expand $w = \sum w_\alpha f_\alpha$.
Notice that $\iota_{f_j}f_{\alpha} = 0$ if $j$ is not a term in $\alpha$. Otherwise, $\iota_{f_j}f_\alpha = \pm f_{\alpha'}$, where $\alpha'$ is the increasing $(k-1)$-tuple obtained from $\alpha$ by removing $j$ (the sign depends on $j$ and $\alpha$). Thus, $\langle \iota_{f_j}f_\alpha,\iota_{f_j}f_\beta\rangle = \delta_{\alpha  = \beta}$, the Kronecker $\delta$, and
\[\norm{\iota_{f_j}w}^2 = \sum_{\alpha,\beta}w_\alpha\overline{w_\beta}\langle \iota_{f_j}e_\alpha,\iota_{f_j}e_\beta\rangle = \sum_{\alpha\ni j} |w_\alpha|^2,\] where the sum ranges over all those $\alpha$ one of whose terms is $j$.
Thus 
\[\sum_j \norm{\iota_{f_j}w}^2 = \sum_{j}\sum_{\alpha \ni j} |w_\alpha|^2.\] In this sum, each term $|w_\alpha|^2$, for an increasing $k$-tuple $\alpha$, appears precisely $k$ times: once for each $j = \alpha_i$, $1 \leq i \leq k$. We conclude that
\[\sum_j \norm{\iota_{f_j}w}^2 = k\sum_{\alpha} |w_\alpha|^2 = k\norm{w}^2 = k.\] Putting it all together, the second factor on the last line of \eqref{CS} is bounded by
$\norm{C(t)}^{j-1}\sqrt{k}$, and recalling the bound on the first factor, \eqref{CS} is bounded by
\[k\norm{A-B}\norm{C(t)}^k.\] Now \[\norm{C(t)} \leq (1-t)\norm{A} + t\norm{B} \leq \max(\norm{A},\norm{B}).\] Hence, from \eqref{ff},
\[\norm{\Lambda^k A - \Lambda^k B} \leq \int_0^1 k\norm{A-B}\max(\norm{A},\norm{B})^k\ dt \leq k\norm{A-B}\max(\norm{A},\norm{B})^{k-1},\] which is the desired bound.


\textbf{Part 5: Trace class operators.} Now let us suppose $A$ is trace class. We prove that $\Lambda^k A$ is trace class, i.e.\ $|\Lambda^k A|$ is trace class. We know that $|\Lambda^k A| = \Lambda^k |A|$, so replacing $A$ with $|A|$, we can assume that $A$ is positive. Since $|A|$ is compact, by the spectral theorem we can find $e_1,e_2,\ldots$, an orthonormal basis of eigenvectors of $A$, and suppose $Ae_i = \lambda_i e_i$. Let $\beta = \{e_{i_1}\wedge \cdots \wedge e_{i_k}\: i_1 < \cdots < i_k\}$. Then 
\begin{align*}
\Tr(\Lambda^k A) &= \sum_{i_1 < \cdots < i_k} \lambda_{i_1}\cdots\lambda_{i_k}\\ 
&= \lim_{n \to \infty}\sum_{i_1 < \cdots < i_k \leq n} \lambda_{i_1}\cdots\lambda_{i_k}\\
&= \lim_{n \to \infty}\sum_{i_1 < \cdots < i_k \leq n}\frac{1}{k!}\sum_{\sigma \in S_k}\lambda_{i_\sigma(1)}\cdots\lambda_{i_\sigma(k)}\\
&= \lim_{n \to \infty}\frac{1}{k!}\sum_{(i_1,\ldots,i_k), i_k \leq n, \text{ has distinct entries }}\lambda_{i_1}\cdots\lambda_{i_k}\\
&\leq \lim_{n \to \infty}\frac{1}{k!}\sum_{(i_1,\ldots,i_k), i_k \leq n, \text{ a }k\text{-tuple}}\lambda_{i_1}\cdots\lambda_{i_k}\\
&= \lim_{n \to \infty}\frac{1}{k!}\left(\sum_{i=1^n} \lambda_i\right)^k\\
&= \frac{\Tr(A)^k}{k!}.
\end{align*}
Thus $A$ is trace class and $\norm{A}_1 \leq \frac{\norm{A}_1^k}{k!}$.

\textbf{Part 6: Continuity in the trace norm.}The proof starts very similarly to part 4, the proof of the continuity in the operator norm, and we use the same notation. Suppose $A$, $B$ are trace-class operators. Let $e_1,e_2,\ldots$ be an orthonormal basis of $H$, and let $\Pi_n$ denote the projection on $\{e_1,\ldots,e_n\}$. Recall that $\Lambda^k \Pi_n$ is the projection onto $\vspan\{e_{i_1}\wedge e_{i_k}\: i_1 < \cdots i_k \leq n\}$. We will show below in \cref{traceconv} that this means that, $\Pi_n A \Pi_n \to A$, $\Pi_n B \Pi_n \to B$, $\Lambda^k \Pi_n A \Pi_n \to \Lambda^k A$, $\Lambda^k \Pi_n B \Pi_n \to \Lambda^k B$, all in the trace norm. Thus, it suffices to prove \eqref{ctsbp} with $A$ and $B$ replaced by $\Pi_n A\Pi_n$ and $\Pi_n B \Pi_n$, respectively. In other words, we may assume that $H$ is finite-dimensional with $\dim H = n$.

Let $C(t)$, $\gamma_{\alpha,v}(t)$, $f_\alpha$, $\lambda_i$ be as in part 4. Write $\Lambda^k A - \Lambda^k B = U|\Lambda^k A - \Lambda^k B|$ for the polar decomposition, so that $|\Lambda^k A - \Lambda^k B| = U^\ast(\Lambda^k A - \Lambda^k B)$. Then, from \eqref{FTC},
\begin{align}
\label{tobeplug}
\begin{split}
\norm{\Lambda^k A - \Lambda^k B}_1 &= |\Tr(U^\ast(\Lambda^k A - \Lambda^k B))|\\
&= \left|\sum_{\alpha} \langle \Lambda^k A - \Lambda^k B)e_{\alpha},Ue_{\alpha}\rangle\right|\\
&\leq \int_0^1 \left|\sum_{\alpha} \gamma_{\alpha,Ue_{\alpha}}'(t)\right|\ dt.
\end{split}
\end{align}
We will bound the integrand uniformly. Plugging in \eqref{der} for the integrand yields
\begin{equation}\label{sum}\sum_{\alpha} \gamma'_{\alpha,Ue_\alpha}(t) = \sum_{\alpha,i}(-1)^{i+1}\lambda_{\alpha_i}\langle  \Lambda^{k-1}C(t)\widehat{e_{\alpha_i}},\iota_{f_{\alpha_i}}Ue_\alpha\rangle.\end{equation} We may rewrite the sum instead over all pairs $(j,\beta)$, where $1 \leq j \leq n$, and $\beta$ is an increasing $(k-1)$-tuple none of whose terms is $j$. To do so, notice that $e_\alpha = (-1)^{i+1}e_{\alpha_i}\wedge \widehat{e_{\alpha_i}}$. Thus, the sum is equal to
\begin{equation}\label{sumtwo}\sum_{\alpha} \gamma'_{\alpha,Ue_\alpha}(t)  = \sum_{j,\beta}\lambda_j\langle \Lambda^{k-1}C(t)e_\beta,\iota_{f_{j}}U(e_j\wedge e_\beta)\rangle.\end{equation} For $j$ fixed, let $U_j:\Lambda^{k-1}H \to\Lambda^{k-1}H$ be the map $w \mapsto \iota_{f_{j}}U(e_j\wedge w)$, which has norm at most $1$.  Let $\Gamma_j\: H \to H$ be the projection off of $e_j$. Then $\Lambda^{k-1}\Gamma_j e_\beta = e_\beta$ precisely when $j$ is not an index in $\beta$, and is $0$ otherwise. Then, for $j$ fixed, the the sum over $\beta$ is just
\[\Tr((\Lambda^{k-1}\Gamma_j U_j^\ast\Lambda^{k-1}C(t)\Lambda^{k-1}\Gamma_j),\]
which is bounded by 
\[\norm{\Lambda^{k-1}\Gamma_j}^2\norm{\Lambda^{k-1}U_j^\ast}\norm{\Lambda^{k-1} C}_1 \leq \frac{\norm{C}_1^{k-1}}{(k-1)!},\]
using the bounds we have proven previously. Thus, \eqref{sumtwo} is bounded by
\begin{align}
\label{dsd}
\begin{split}
\left|\sum_{\alpha} \gamma'_{\alpha,Ue_\alpha}(t)\right| = \left|\sum_j \lambda_j \Tr((\Lambda^{k-1}\Gamma_j U_j^\ast\Lambda^{k-1}C(t)\Lambda^{k-1}\Gamma_j)\right| &\leq \left(\sum_j \lambda_j\right)\frac{\norm{C(t)}_1^{k-1}}{(k-1)!} \\
&= \norm{A-B}_1\frac{\norm{C(t)}_1^{k-1}}{(k-1)!.}
\end{split}
\end{align}
However, \[\norm{C(t)}_1 \leq (1-t)\norm{A}_1 + t\norm{B}_1 \leq \max(\norm{A}_1,\norm{B}_1).\] Therefore, returning to \eqref{tobeplug} and using \eqref{dsd}
\begin{align*}
\norm{\Lambda^k A - \Lambda^k B} &\leq \int_0^1 \left|\sum_{\alpha} \gamma_{\alpha,Ue_{\alpha}}'(t)\ dt\right|\ dt\\
&\leq \int_0^1  \norm{A-B}_1\frac{\max(\norm{A}_1,\norm{B}_1)^{k-1}}{(k-1)!} dt\\
&\leq  \norm{A-B}_1\frac{\max(\norm{A}_1,\norm{B}_1)^{k-1}}{(k-1)!},\end{align*} which is the desired bound.
\end{proof}

We now prove the lemma about convergence in the trace class, which we will also use later.

\begin{lem}\label{traceconv}Let $H$ be a Hilbert space, and let $E_1 \subseteq E_2 \subseteq \cdots$ be a family of strictly increasing finite-dimensional subspaces of $H$ whose closure is dense. Let $\Pi_i$ be the projection on $E_i$. Then $A(1-\Pi_n) \to 0$ and $(1-\Pi_n)A \to 0$ in the trace class norm. In particular, $\Pi_n A \Pi_n \to A$ in the trace-class norm.\end{lem}
\begin{proof}
The second claim follows from the first by bounding
\[\norm{\Pi_n A \Pi_n-A}_1 \leq \norm{(\Pi_n-1)A}\norm{\Pi_n} + \norm{A(\Pi_n-1)}_1.\] The statement for $(1-\Pi_n)A$ follows from that for $A(1-\Pi_n)$ by taking adjoints.

Write $A = U|A|$ and $A(1-\Pi_n) = V|A(1-\Pi_n)|$ for the polar decompositions. Then
\[|A(1-\Pi_n)| = V^\ast U|A|(1-\Pi_n) = (V^\ast U |A|^{1/2})(|A|^{1/2}(1-\Pi_n)).\] Set $W = V^\ast U$. We may pick an orthonormal basis $\{e_1,\ldots,e_{m_1}\}$ of $E_1$, extend it to an orthonormal basis $\{e_1,\ldots,e_{m_2}\}$ of $E_2$, etc, obtaining an orthonormal basis $e_1,e_2,\ldots$ of $H$, such that for $m_i = \dim E_i$, $\{e_1,\ldots,e_{m_i}\}$ is an orthonormal basis for $E_i$. Then
\begin{align*}
\norm{A(1-\Pi_n)}_1 &= |\Tr(|A(1-\Pi_n)|)| = \left|\sum_{i=1}^{\infty} \langle |A|^{1/2}(1-\Pi_n)e_i,|A|^{1/2}W^\ast e_i\rangle\right|\\
&= \left|\sum_{i=m_n}^{\infty} \langle |A|^{1/2}e_i,|A|^{1/2}W^\ast e_i\rangle\right|\\
&\leq \left(\sum_{i=m_n}^{\infty}\norm{|A|^{1/2}e_i}^2\right)^{1/2} \left(\sum_{i=m_n}^{\infty}\norm{|A|^{1/2}W^\ast e_i}^2\right)^{1/2}.\end{align*}
The square of the second factor is bounded, uniformly in $n$, by 
\[\sum_{i=1}^{\infty}\norm{|A|^{1/2}W^\ast e_i}^2 = \Tr(W|A|W^\ast) \leq \norm{A}_1.\] The square of the first factor is
\[\sum_{i=m_n}^\infty\langle \langle|A|e_i,e_i\rangle,\] which goes to $0$ as $n \to \infty$.
\end{proof}


\section{The Fredholm determinant}
We can now define the Fredholm determinant.
\begin{defn}Suppose $A:H \to H$ is trace class. Then define
\[\det\nolimits_{\mathrm{Frd}}(1+A) = \sum_{k=0}^\infty \Tr(\Lambda^k A),\]\end{defn}
interpreting $\Tr(\Lambda^0 A) = 1$. This makes sense since by \cref{extends} $|\Tr(\Lambda^k A)| \leq \frac{\norm{A}_1^k}{k!}$ for all $k$, and hence the defining series is absolutely summable.

Let us check that this agrees with the usual definition in the case that $H$ is finite-dimensional. In fact,
\begin{prop}\label{findet}Suppose $K \subseteq H$ is finite-dimensional, and $A = \Pi A \Pi$, where $\Pi$ is the orthogonal projection onto $K$. Then, with $\det\nolimits_{\mathrm{us}}$ interpreted as the usual determinant of a linear map between fininite dimensional spaces,
\[\det\nolimits_{\mathrm{us}}((1+A)|_K) = \det\nolimits_{\mathrm{Frd}}(1+A).\]\end{prop}
\begin{proof}
Suppose $\dim K = n$. Fix $k > n$, and a $k$-blade $v = v_1\wedge \cdots \wedge v_k$. Write $v_i = u_i + w_i$, where $u_i \in K$ and $w_i \bot K$. Then $v = u + w$, where $u$ is a wedge of $k+1$ vectors in $K$, and is hence $0$, and $w$ is a sum of wedges of terms such as at least one constituent factor per term is perpendicular to $K$. So $\Lambda^kAv = 0 + \Lambda^k Aw = 0$. So $\Lambda^k A \equiv 0$ if $k > n$. Therefore the sum $\sum_{k=0}^\infty \Tr(\Lambda^k A)$ only goes up to $k = n$. Suppose $e_1,\ldots,e_n,e_{n+1},\ldots$ is an orthonormal basis of $H$ such that $e_1,\ldots,e_n$ is an orthonormal basis of $K$. Reall that
\[\Lambda^n (1+A)e_1\wedge \cdots \wedge e_n = \det\nolimits_{\mathrm{us}}((1+A)|_K)e_1\wedge \cdots \wedge e_n.\]
On the other hand
\[\Lambda^n (1+A)e_1\wedge \cdots \wedge e_n = (1+A)e_1 \wedge \cdots \wedge (1+A)e_n.\] In the expansion wedge product, each term is a wedge of factors of the form $Ae_i$ or $e_j$. Set $B^0 = 1$ and $B^1 = A$. Let $\sigma \subseteq \{1,\ldots, n\}$, and interpret $\sigma:\{1,\ldots n\} \to \{0,1\}$, where $\sigma(i) = 1$ if $i \in \sigma$. Then
\[\Lambda^n (1+A)e_1\wedge \cdots \wedge e_n = \sum_{\sigma} B^{\sigma(1)}e_1 \wedge \cdots \wedge B^{\sigma(n)} e_n.\]
For a fixed $\sigma = \{i_1,\cdots, i_k\} \subseteq \{1,\ldots, n\}$, with $\{1,\ldots, n\}\setminus \sigma = \{j_{k+1},\ldots, j_n\}$, the corresponding term above is equal to
\begin{equation}\label{plugin}\pm Ae_{i_1}\wedge \cdots Ae_{i_k}\wedge e_{j_{k+1}}\wedge \cdots\wedge e_{ j_n} = \pm(\Lambda^k A)(e_{i_1}\wedge \cdots \wedge e_{i_k})\wedge(e_{j_{k+1}}\wedge \cdots\wedge e_{j_n}),\end{equation}
with the sign $\pm$ depending on how many swaps are required to turn $e_{\sigma(1)}\wedge \cdots \wedge e_{\sigma(n)}$ into $e_{i_1}\wedge \cdots \wedge e_{i_k} \wedge e_{j_{k+1}} \wedge \cdots \wedge e_{j_n}$.
Let us assume without loss of generality that $i_1 < \cdots < i_k$, $j_1 < \cdots < j_k$. Expanding in an orrthonormal basis, we may write
\begin{equation}\label{expands}(\Lambda^k A)(e_{i_1}\wedge \cdots \wedge e_{i_k}) = \sum_{\ell_1 < \cdots < \ell_k} \langle (\Lambda^k A)(e_{i_1}\wedge \cdots \wedge e_{i_k}), e_{\ell_{1}}\wedge\cdots\wedge e_{\ell_k}\rangle e_{\ell_{1}}\wedge\cdots\wedge e_{\ell_k}.\end{equation} Let us examine the term corresponding to $\{\ell_1 < \cdots < \ell_k\}$ in this sum. If any $\ell_p > n$, then this term is $0$, since $A$ is $0$ on the orthocomplement to $K$. If $\ell_p = j_r$ for some $p$ and $r$, then the wedge product of this term with $e_{j_{k+1}}\wedge \cdots\wedge e_{ j_n}$ is $0$. Thus the only term in \eqref{expands} which survives after wedging with $e_{j_{k+1}}\wedge \cdots\wedge e_{ j_n}$ is the term corresponding to $\ell_p = i_p$ for all $p$. Plugging \eqref{expands} into \eqref{plugin} and using this fact yields
\begin{align*}
&\pm Ae_{i_1}\wedge \cdots Ae_{i_k}\wedge e_{j_{k+1}}\wedge \cdots\wedge e_{ j_n}\\
& = \pm\langle(\Lambda^k A)(e_{i_1}\wedge \cdots \wedge e_{i_k}), e_{i_{1}}\wedge\cdots\wedge e_{i_k}\rangle e_{i_1}\wedge \cdots \wedge e_{i_k} \wedge e_{j_{k+1}} \wedge \cdots \wedge e_{j_n}\\
&= \langle(\Lambda^k A)(e_{i_1}\wedge \cdots \wedge e_{i_k}), e_{i_{1}}\wedge\cdots\wedge e_{i_k}\rangle e_1\wedge \cdots \wedge e_n.\end{align*}

Since
\[\langle(\Lambda^k A)(e_{\ell_1}\wedge \cdots \wedge e_{\ell_k}), e_{\ell_{1}}\wedge\cdots\wedge e_{\ell_k}\rangle = 0\] if any $\ell_p > n$,
summing
\[\langle(\Lambda^k A)(e_{i_1}\wedge \cdots \wedge e_{i_k}), e_{i_{1}}\wedge\cdots\wedge e_{i_k}\rangle e_1\wedge \cdots \wedge e_n\]
over all subsets $\sigma = \{i_1 < \cdots < i_k\} \subseteq \{1,\ldots, n\}$ is the same as summing it over all sets $\{i_1 < \cdots < i_k\} \subseteq \N$, and thus the sum equals
\[\Tr(\Lambda^k A)e_1 \wedge \cdots \wedge e_n.\]

Recalling the definition of $B^j$, we have thus shown that 
\[\sum_{\#\sigma = k}B^{\sigma(1)}e_1 \wedge \cdots \wedge B^{\sigma(n)} e_n = \Tr(\Lambda^k A)e_1 \wedge \cdots \wedge e_n.\]
The sum of this over all $k \leq n$ is thus on the one had equal to $\det\nolimits_{\mathrm{us}}((1+A)|_K)e_1 \wedge \cdots \wedge e_n$, as we have shown, and is on the other hand equal to 
$\left(\sum_{k=0}^n \Tr(\Lambda^k A)\right)e_1 \wedge \cdots \wedge e_n = \det_{\mathrm{Frd}}(1+A)$.\end{proof}


We will use \cref{findet} to approximate the Fredholm determinant of an operator by finite-rank approximations. Fortunately, we have \cref{traceconv} which will guarantee that the finite-dimensional approximations converge in the trace-class norm. Using the continuity of $\Lambda^{k}:\ell_1(H) \to \ell_1(\Lambda^{k} H)$ will allow us to show that the Fredholm determinant is continuous, and thus the finite-dimensional approximations converge. Indeed:
\begin{lem}\label{detcont}The Fredholm determinant is continuous in the trace-class norm. Explicitly, if $A$ and $B$ are trace class, then
\[|\det(1+A)-\det(1+B)| \leq \norm{A-B}_1\exp(\max(\norm{A}_1,\norm{B}_1)).\]
\end{lem}
\begin{proof}
This follows easily from \cref{extends}. Indeed,
\[|\det(1+A)-\det(1+B)| \leq \sum_{k>1} |\Tr(\Lambda^k A - \Lambda^k B)| \leq \sum_{k > 1}\norm{A-B}_1\frac{\max(\norm{A}_1,\norm{B}_1)^{k-1}}{(k-1)!}\]
(the $k=0$ term vanishes since $\Tr(\Lambda^0 A) = \Tr(\Lambda^0 B) = 1$). The lemma follows.
\end{proof}

\begin{thm}[Properties of the determinant]
Suppose $A$, $B$ are trace class. Then
\begin{romanumerate}
\item $\det(1+A^\ast) = \overline{\det(1+A)}$;
\item $\det(1+A)\det(1+B) = \det((1+A)(1+B))$;
\item if $A$ is self-adjoint with eigenvalues $\lambda_1, \lambda_2, \ldots$, then $\det(1+A) = \prod_{i=1}^{\infty} (1+\lambda_i)$;
\item if $X$ is invertible, then $\det(1+XAX^{-1}) = \det(1+A)$;
\item $\det(1+A) = 0$ if and only if $1+A$ is not invertible;
\item $\exp(A)-1$ is trace class and
$\det(\exp(A)) = \exp(\Tr(A))$.
\end{romanumerate}
\end{thm}
\begin{proof}
Let $e_1,e_2$ be an orthonomal basis for $H$ and let $\Pi_n$ be the orthogonal projection onto $\vspan\{e_1,\ldots,e_n\}$.

Let us first prove (i). For each $n$, oberserve that \[((1+\Pi_n A \Pi_n)|_{\range(\Pi_n)})^\ast = (1+\Pi_n A^\ast \Pi_n)|_{\range(\Pi_n)}.\] It follows from \cref{findet} that
\begin{align}
\label{conv1}
\begin{split}\det(1+\Pi_n A^\ast \Pi_n) &= \det\nolimits_{\mathrm{us}}(((1+\Pi_n A \Pi_n)|_{\range(\Pi_n)})^\ast)\\
&=  \overline{\det\nolimits_{\mathrm{us}}((1+\Pi_nA\Pi_n|_{\range(\Pi_n)})} = \overline{\det(1+\Pi_nA\Pi_n)}.\end{split}\end{align}
By \cref{traceconv}, $\Pi_n A \Pi_n$, and $\Pi_n A^\ast \Pi_n$ converge to $A$ and $A^\ast$, respectively, in the trace class norm, and thus by \cref{detcont}, $\det(1+\Pi_n A \Pi_n) \to \det(1+A)$ and similarly $\det(1+\Pi_n B \Pi_n) \to \det(1+B)$. Taking limits in \eqref{conv1} proves (i).

Now let us show (ii). Again from \cref{findet}, for $n \geq N$
\[\det(1+\Pi_n A\Pi_n)\det(1+\Pi_nB\Pi_n) = \det(1+\Pi_nA\Pi_n + \Pi_nB\Pi_n + \Pi_nA\Pi_nB\Pi_n).\] As above, the left-hand side converges to $\det(1+A)\det(1+B)$. For the right-hand side, we know that $\Pi_nA\Pi_n$ and $\Pi_nB \Pi_nB$ converge to $A$ and $B$ in the trace-class norm, so to establish that the right-hand side converges to $\det(1+A+B+AB) = \det((1+A)(1+B))$, we just need to show that $\Pi_nA\Pi_nB\Pi_n \to AB$ in the trace-class norm. Indeed, we may bound
\[\norm{\Pi_nA\Pi_nB\Pi_n-AB}_1 \leq \norm{(\Pi_n-1)A}_1\norm{\Pi_nB\Pi_n} + \norm{A(\Pi_n-1)}_1\norm{B\Pi_n} + \norm{A}\norm{B(\Pi_n-1)}_1 \to 0.\]

Now let us show (iii). Assume without loss of generality that $e_1,e_2 \cdots$ are eigenvectors of $A$, and that $Ae_i = \lambda_ie_i$. Then from \cref{findet}
\[\det(1+\Pi_nA\Pi_n) = \prod_{i=1}^n (1+\lambda_i).\] Taking $n \to \infty$ as usual (and using that $\sum |\lambda_i| < \infty$) shows (iii).

Next let us show (iv). Let $K_n = \range(\Pi_n)$, and let $\Gamma_n$ be the orthogonal projection onto $K_n' = X(K_n)$. We know from \cref{findet} that
\begin{align*}
\det(1+\Pi_n A \Pi_n) &= \det\nolimits_{\mathrm{us}}((1+\Pi_nA\Pi_n)|_{K_n})\\
 &= \det\nolimits_{\mathrm{us}}(X|_{K_n}(1+\Pi_nA\Pi_n)|_{K_n}X^{-1}|_{K_n'})\\
 & = \det\nolimits_{\mathrm{us}}t((1+X\Pi_n A\Pi_nX^{-1})|_{K_n'})\\
 &= \det(1+(\Gamma_nX\Pi_nA\Pi_nX^{-1}\Gamma_n)|_{K_n'})
 &= \det(1+\Gamma_nX\Pi_nA\Pi_nX^{-1}\Gamma_n).\end{align*}
As usual, the left-hand side converges to $\det(1+A)$, and the right-hand side converges to $\det(1+XAX^{-1})$ provided  $T_n := \Gamma_nX\Pi_nA\Pi_nX^{-1}\Gamma_n$ converges in the trace-class norm to $XAX^{-1}$. As in the proof of \cref{traceconv}, we may take an orthornormal basis $f_1,f_2,\ldots$ such that $f_1,\ldots,f_n$ is a basis of $K_n'$, and thus $\Gamma_n$ is the orthogonal projection onto $\{f_1,\ldots,f_n\}$. Observe that by definition $\Gamma_nX\Pi_n = X\Pi_n$. Therefore
\begin{align*}
\norm{T_n - XAX^{-1}}_1 \leq &\norm{X}\norm{(\Pi_n-1)A}_1\norm{\Pi_nX^{-1}\Gamma_n}\\
&+ \norm{X}\norm{A(\Pi_n-1)}_1\norm{X^{-1}\Gamma_n} + \norm{X}\norm{AX^{-1}(1-\Gamma_n)}_1 \to 0\end{align*}
(recall that $AX^{-1}$ is trace class). This shows (iv).

Finally we show (v). Suppose $1+A$ is not invertible. Since $1+A$ is Fredholm of index $0$, it follows that $1+A$ has closed range, and $\dim \ker (1+A) = \dim \range(1+A)^\bot$. In particular, $1+A$ has a null space containing at least one unit-norm vector $e_1$. Extend $e_1$ to an orthonormal basis $e_1,e_2,\ldots$ of $H$. Let $\Pi_n$ be the projection onto $e_1,\ldots,e_n$. By assumption, $Ae_1 = -e_1$. Thus $\Pi_n A\Pi_n e_1 = -e_1$, and so $(1+\Pi_n A \Pi_n)e_1 = 0$. Thus $0 = \det(1+\Pi_n A \Pi_n)$. As usual, this converges to $\det(1+A)$, which shows that it is $0$.

Now suppose $\det(1+A) = 0$. Then, by (i), $\det(1+A^\ast) = 0$, and so by (ii), $\det((1+A)^\ast(1+A)) = 0$, and thus $\det(1+ (A^\ast A + A^\ast + A)) = 0$. Write $(A^\ast A + A^\ast + A) = P$. Then $P$ is self-adjoint, $P$ is trace class, and $\det(1+P) = 0$. Thus, by (iii), $\prod_{i=1}^{\infty}(1+\lambda_i) = 0$, where $\lambda_i$ are the eigenvalues of $P$. If none of the $\lambda_i$ were $-1$, then since $\sum |\lambda_i| < \infty$ (since $P$ is traceclass), $\prod_{i=1}^{\infty} (1+\lambda_i) \neq 0$. Thus, at least one of the $\lambda_i = 0$, and so $1+P$ has non-trivial kernel, and hence $1+A$ does, too.

Now let us show (vi). By definition
\[\exp(A)-1 = \sum_{k=1}^{\infty} \frac{A^k}{k!}.\] Since $\norm{A^k}_1 \leq \norm{A^{k-1}}\norm{A}_1$, this sum converges absolutely in the trace class norm, and thus converges to a trace-class operator. From \cref{findet} and properties of the validity of the formula in finite dimensions,
\[\det(\exp(\Pi_nA\Pi_n)) = \exp(\Tr(\Pi_nA\Pi_n)).\] From \cref{traceconv}, the right-hand side converges. To show the left-hand side converges, we need to show that $\norm{\exp(A)-1-(\exp(\Pi_nA\Pi_n)-1)}_1 \to 0$. By definition, we may control this by
\[\sum_{k=1}^{\infty} \frac{\norm{(\Pi_nA\Pi_n)^k-A^k}_1}{k!} = \sum_{k=1}^{\infty} \frac{\norm{(\Pi_nA)^k\Pi_n-A^k}_1}{k!}.\] Let us control the numerator of each term. With the usual trick, one has
\begin{align*}
\norm{(\Pi_nA)^k\Pi_n-A^k}_1 &\leq \sum_{j=0}^{k-1} \norm{A}^j\norm{(\Pi_n-1)A}_1\norm{\Pi_nA}^{k-j-1} + \norm{A}^{k-1}\norm{A(1-\Pi_n)}_1\\
&\leq (k+1)\norm{A}^k\max(\norm{(1-\Pi_n)A}_1,\norm{A(1-\Pi_n)}_1).\end{align*}
Therefore
\[\norm{\exp(A)-\exp(\Pi_nA\Pi_n)}_1 \leq \max(\norm{(1-\Pi_n)A}_1,\norm{A(1-\Pi_n)}_1)\sum_{k=1}^{\infty} \frac{(k+1)\norm{A}^k}{k!}).\]The sum converges, and the factor out front converges to $0$ by \cref{traceconv}, which proves the claim.


\end{proof}

Let us end this note by briefly addressing derivatives. Suppose $a<b \in \R$ and $A(t)$, $t \in [a,b]$ is a family of trace-class operators, differentiable at $t=t_0$,\footnote{Here, differentiability means that there exists a trace class $A'(t_0)$ such that $A(t_0+h)-A(t_0) = A'(t_0) + R_h$, where $\norm{R_h}_1 \in o(h)$}.
\begin{prop}[Jacobi's formula]If $1+A(t_0)$ is invertible, then $\det(1+A(t))$ is differentiable at $t=t_0$ and
\[\det(1+A(t))'|_{t = t_0} = \det(1+A(t_0))\Tr((1+A(t_0))^{-1}A'(t_0)).\]\end{prop}
\begin{proof}
Without loss of generality, let us assume that $t_0 = 0$. To start off, let us take the special case $A(t) = tB$, for some trace-class $B$. Then $A'(0) = B$. By definition,
\[\det(1+tB) = \sum_{k=0}^{\infty}\Tr(\Lambda^k tB).\]
Testing on $k$-blades, it is clear that $\Lambda^k tB = t^k\Lambda^k B$. Therefore,
\[|\det(1+tB)-\det(1+0)-\Tr(B)| \leq t^2\sum_{k=2}^{\infty}t^{k-2}\Tr(\Lambda^k B) \leq t^2\left(\sum_{k=2} \frac{\norm{B}_1^k}{k!}\right),\] which is certainly in $o(t)$ as $t \to 0$.
Now assume $A(t)$ is some aribtrary curve differentiable at $0$. Since $A(t)$ is differentiable, we may write $A(t) = A(0) + tA'(0) + R_t$, where $\norm{R_t}_1 \in o(t)$. Thus,
\begin{align*}
(1+A(0))^{-1}(1+A(t)) &=  (1+A(0))^{-1}(1+A(0) + tA'(0) + R_t)\\
&= 1 + t(1+A(0))^{-1}A'(0) + (1+A(0))^{-1}R_t\end{align*}
 is of the form $1+K$, where $K$ is trace-class. In particular
\begin{align*}\det(1+A(t)) &= \det((1+A(0))(1+A(0))^{-1}(1+A(t)))\\
&= \det(1+A(0))\det(1 + t(1+A(0))^{-1}A'(0) + (1+A(0))^{-1}R_t).\end{align*}
By \cref{detcont}, 
\begin{align*}
|\det(1 + t(1+A(0))^{-1}A'(0) + (1+A(0))^{-1}R_t) - &\det(1+t(1+A(0))^{-1}A'(0))|\\
&\leq \norm{(1+A(0))^{-1}}o(t)\exp(C_t),\end{align*}
where
\begin{align*}
C_t &= \max(\norm{t(1+A(0))^{-1}A'(0) + (1+A(0))^{-1}R_t}_1\norm{t(1+A(0))^{-1}A'(0)}_1)\\
&\leq t\norm{(1+A(0))^{-1}}_1(\norm{A'(0)}_1 + o(1))\end{align*}
is uniformly bounded as $t\to 0$.
This shows that \[|\det(1 + t(1+A(0))^{-1}A'(0) + (1+A(0))^{-1}R_t) - \det(1+t(1+A(0))^{-1}A'(0))| \in o(t),\] and so
\begin{align*}
&\det(1 + t(1+A(0))^{-1}A'(0) + (1+A(0))^{-1}R_t) -1 - \Tr((1+A(0))^{-1}A'(0))\\
&= \det(1+t(1+A(0))^{-1}A'(0)) - 1-\Tr((1+A(0))^{-1}A'(0)) + o(t).\end{align*}
But by the special case, this is just in $o(t)$. Thus,
$\det((1+A(0))^{-1}(1+A(t))$ is differentiable with derivative $\Tr((1+A(0))^{-1}A'(0))$, and so
\[\det(1+A(t)) = \det((1+A(0))\det((1+A(0))^{-1}(1+A(t))\] iss differentiable, too, with derivative
\[\det((1+A(0))\Tr((1+A(t_0))^{-1}A'(0)),\] as desired.\end{proof}


\end{document}


